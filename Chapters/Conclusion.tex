
\chapter{Conclusion}

The original goal of this thesis and oxiflex was to showcase how much of an impact forward checking and arc consistency would make. Altough they did have an impact, the impact is in the wrong direction. Enforcing arc consistency made oxiflex in some cases slower by an order of magnitude. At least forward checking made the N-Queens problem be solved faster.

It is great to see the tradeoff between search and inference though. As we could see in~\cref{fig:slow:sidebyside}, altough it took longer to solve, inference did reduce the number of iterations significantly. It is interesting to see the effect that variable ordering has for the slow convergence problem. In fact it made it even possible to solve the problem at all. It can be useful to measure other things than time (like iterations) to gather insights like these.

The performance measured at the point of writing this is already significantly better compared to previous versions of oxiflex. Mainly by removing values from domains inplace and using the \verb|constraint_index| hashmap to only check constraints that are needed. Making improvements like this included trial and error, benchmarking and profiling. Making a change like using a different datastructure could feel like it could improve performance but without measuring before and after, the change can make the code more complex without providing performance gain.

Currently the main performance loss for arc consistency enforcing algorithms in oxiflex lie within the \verb|revise| function. After profiling the main issue seemes to be around creating hashmaps that act as \verb|PartialAssignment|. It seemes that altough hashmaps have an insertion complexity of $\mathcal{O}(1)$ that the most time within \verb|revise| was spent on creation of and insertions made to the hashmap used to then check constraints. It feels like a good idea to not use hashmaps and just pass the needed data directly to be checked within a simple struct. This is easily possible to refactor because within \verb|revise| only two variables are checked by their constraints. But this modification turned out to make it worse and degrade the performance even further.

Both inference methods repeatedly go over domains of variables and remove values. Domains are implemented using a simple \verb|Vec|. As already mentioned those are pointer, capacity and length triplets. So removing a single element has complexity of $\mathcal{O}(n)$. As we remove values from domains often, it would make sense to use a \verb|LinkedList| instead as it has complexity of $\mathcal{0}(1)$ for removing a single element. After trying this out, the performance stayed the same. Altough removing elements from \verb|Vec| takes $\mathcal{O}(n)$ we always have to go every value in a domain anyways to check all values against the constraints. Not on purpose this removal of values in domains was done using the function \verb|retain|. \verb|retain| accepts a predicate and goes over all values in the vec and only keeps values that the predicate allows. After benchmarking this was just as efficient as using \verb|LinkedList| for domains.

Therefore the main takeaway for this thesis is that datastructures matter. Altough an algorithm performs better in theory, the right datastructures have to be used to make it really go faster. Using hashmaps for everything migth not be the best approach if performance is the main criteria of a program. This also underlines that just using a fast programming language is not sufficient to make a program go fast.

